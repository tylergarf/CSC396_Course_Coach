{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a136d87f-81e8-4e65-9d30-75fcd2c6ff7b",
   "metadata": {},
   "source": [
    "# Performing needed imports and boilder plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472cc83c-1425-47ed-b290-9039207e03ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m random.seed(seed)\n\u001b[32m     13\u001b[39m np.random.seed(seed)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtorch\u001b[49m.manual_seed(seed)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     16\u001b[39m     torch.cuda.manual_seed(seed)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set this variable to a number to be used as the random seed\n",
    "# or to None if you don't want to set a random seed\n",
    "seed = 1234\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b032e-e4dc-4cb0-96e2-5b5acaeb463c",
   "metadata": {},
   "source": [
    "# Copied over data cleaning steps from our data cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7721-2d24-48e9-bd40-e66aea329f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_data = pd.read_csv('students_clean.csv')\n",
    "\n",
    "\n",
    "education_data.drop('Parent_Education_Level', axis=1, inplace=True) \n",
    "\n",
    "education_data['Gender'] = education_data['Gender'].replace({'Male': 1, 'Female': 0}).astype(int)\n",
    "education_data['Internet_Access_at_Home'] = education_data['Internet_Access_at_Home'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "education_data['Extracurricular_Activities'] = education_data['Extracurricular_Activities'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "\n",
    "# Low = 1, Medium = 2, High = 3\n",
    "mapper = {'low': 1, 'medium': 2, 'high': 3}\n",
    "\n",
    "education_data['Family_Income_Level'] = (\n",
    "    education_data['Family_Income_Level']\n",
    "      .astype(str)                  # works even if the value is already 1/2/3 or NaN\n",
    "      .str.strip().str.lower()\n",
    "      .map(mapper)                  # returns NaN where no mapping found\n",
    "      .fillna(education_data['Family_Income_Level'])  # keepin the original numeric/blank entries\n",
    "      .astype('Int64')              #  nullable integer dtype\n",
    ")\n",
    "\n",
    "labels = open('departments.txt').read().splitlines()\n",
    "department_mapping = {name: index for index, name in enumerate(labels)}\n",
    "department_indices = education_data['Department'].map(department_mapping)\n",
    "education_data.insert(3, 'department index', department_indices)\n",
    "\n",
    "mapper = {'A': 1, 'B': 1, 'C': 1, 'D':0,'F':0}\n",
    "\n",
    "education_data['Grade'] = (\n",
    "    education_data['Grade']\n",
    "      .astype(str)              # convert everything to string\n",
    "      .str.strip().str.upper()  # remove spaces and standardize to uppercase\n",
    "      .map(mapper)              # map letters to numbers\n",
    ")\n",
    "\n",
    "education_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd384bde-8776-4715-be27-1e4987b0464a",
   "metadata": {},
   "source": [
    "# Now defining our data loader and perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae6abf-6783-4b0e-a3e9-7b40b2700d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        x = torch.tensor(row[self.feature_cols].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target_col], dtype=torch.long)  # long for classification\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220d113-16ca-4e46-b771-b8fd567195e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(train_dl, n_features, pos_class):\n",
    "    # First initialize the model.\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    n_errors = 0\n",
    "    weight_steps = []\n",
    "    total_pos_in_train = 0\n",
    "\n",
    "    # Adding this in for debug purposes to track the changes to the weight vectors on each\n",
    "    # round.\n",
    "    \n",
    "    # Average perceptron features\n",
    "    totalW = np.zeros(n_features)\n",
    "    totalB = 0;\n",
    "    updateCount = 0;\n",
    "    \n",
    "    # Now loop through each batch.\n",
    "    for batch_idx, (x, y) in tqdm(enumerate(train_dl), total=len(train_dl),):\n",
    "        \n",
    "        x_curr_np = x.numpy()\n",
    "        y_curr_np = y.numpy()\n",
    "\n",
    "        total_pos_in_train += (y_curr_np == 1).sum(axis=0)\n",
    "        \n",
    "\n",
    "        # Now perform the training/classification loop.\n",
    "        scores = x_curr_np @ w + b\n",
    "       \n",
    "        \n",
    "        y_pred = (scores > 0).astype(int)\n",
    "\n",
    "\n",
    "        # Now we vectorize the update to make this more efficient.\n",
    "        pred_error = y_curr_np - y_pred\n",
    "        n_errors += np.sum(np.abs(pred_error) != 0) # If the pred error is zero then it is correct.\n",
    "\n",
    "        # First append the previous weights to weight steps which will be used for debuging puprposes.\n",
    "        weight_steps.append((pred_error[:,None]*x_curr_np).sum(axis=0).copy())\n",
    "        \n",
    "        w += (pred_error[:,None]*x_curr_np).sum(axis=0) # Re-shape pred errors to update and only add\n",
    "                                                        # inccorect preds, axis=0 for rows.\n",
    "        b += pred_error.sum()\n",
    "\n",
    "        # Now print out the weights and bias updates every update if we are in debug mode.\n",
    "        \n",
    "\n",
    "    # Now once we are done training the result is the weights and biases.\n",
    "    return (w,b,n_errors,weight_steps.copy(),total_pos_in_train) # I am just copying to avoid weird cases due to mutability of list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c0939-c452-47a5-a973-7007d82813a2",
   "metadata": {},
   "source": [
    "# Create the training and testing partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92df580-200c-4b7d-943e-21adc213eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(education_data, train_size=0.9,random_state=seed)\n",
    "train_df,dev_df = train_test_split(train_df, train_size=0.8,random_state=seed)\n",
    "\n",
    "train_df.reset_index(inplace=True,drop=True)\n",
    "dev_df.reset_index(inplace=True,drop=True)\n",
    "test_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "print(f'train rows: {len(train_df.index):,}')\n",
    "print(f'dev rows: {len(dev_df.index):,}')\n",
    "print(f'test rows: {len(test_df.index):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae62e5f-bc8b-4386-99e1-bbf597d4d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pass/fail distribution in train and dev datasets\n",
    "print(\"=\"*70)\n",
    "print(\"Pass/Fail Distribution in Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train set distribution\n",
    "train_pass = (train_df['Grade'] == 1).sum()\n",
    "train_fail = (train_df['Grade'] == 0).sum()\n",
    "train_total = len(train_df)\n",
    "\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  Pass (1): {train_pass} ({train_pass/train_total*100:.2f}%)\")\n",
    "print(f\"  Fail (0): {train_fail} ({train_fail/train_total*100:.2f}%)\")\n",
    "print(f\"  Total: {train_total}\")\n",
    "\n",
    "# Dev set distribution\n",
    "dev_pass = (dev_df['Grade'] == 1).sum()\n",
    "dev_fail = (dev_df['Grade'] == 0).sum()\n",
    "dev_total = len(dev_df)\n",
    "\n",
    "print(f\"\\nDev Set:\")\n",
    "print(f\"  Pass (1): {dev_pass} ({dev_pass/dev_total*100:.2f}%)\")\n",
    "print(f\"  Fail (0): {dev_fail} ({dev_fail/dev_total*100:.2f}%)\")\n",
    "print(f\"  Total: {dev_total}\")\n",
    "\n",
    "# Test set distribution\n",
    "test_pass = (test_df['Grade'] == 1).sum()\n",
    "test_fail = (test_df['Grade'] == 0).sum()\n",
    "test_total = len(test_df)\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Pass (1): {test_pass} ({test_pass/test_total*100:.2f}%)\")\n",
    "print(f\"  Fail (0): {test_fail} ({test_fail/test_total*100:.2f}%)\")\n",
    "print(f\"  Total: {test_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9940c6-6dc7-4917-be9d-ac08f8c50b4c",
   "metadata": {},
   "source": [
    "# Before Midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148d38e-4f37-4f1e-8028-7ad1e564ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged Perceptron: Train 10 perceptrons and average their weights\n",
    "n_perceptrons = 10\n",
    "weight_vecs = []\n",
    "bias_vecs = []\n",
    "features_lst = ['Attendance (%)', \\\n",
    "       'Assignments_Avg', 'Quizzes_Avg', \\\n",
    "       'Participation_Score', 'Internet_Access_at_Home', \\\n",
    "       'Stress_Level (1-10)', \\\n",
    "       'Sleep_Hours_per_Night', 'Extracurricular_Activities']\n",
    "num_feat = len(features_lst)\n",
    "batch_size=64\n",
    "shuffle = True\n",
    "\n",
    "for i in range(n_perceptrons):\n",
    "    train_ds = MyDataset(train_df, features_lst, 'Grade')\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    w_curr, b_curr, error_curr, weight_hist_curr, tot_train_pos_curr = train_perceptron(train_dl, num_feat, pos_class=1)\n",
    "    \n",
    "    weight_vecs.append(w_curr)\n",
    "    bias_vecs.append(b_curr)\n",
    "\n",
    "# Average the weights and biases\n",
    "w_avg = np.mean(weight_vecs, axis=0)\n",
    "b_avg = np.mean(bias_vecs, axis=0)\n",
    "\n",
    "print(f\"-------------------Averaged Perceptron (10 perceptrons)------------------------------\\n\")\n",
    "print(f\"Averaged weight vector shape: {w_avg}\")\n",
    "print(f\"Averaged bias value: {b_avg:.4f}\\n\")\n",
    "\n",
    "# Test on dev.\n",
    "X_dev_a = dev_df[features_lst].to_numpy()\n",
    "dev_y_true = dev_df['Grade'].to_numpy()\n",
    "dev_y_pred = ((X_dev_a @ w_avg + b_avg) > 0).astype(int)\n",
    "n_correct_dev = (dev_y_true==dev_y_pred).sum(axis=0)\n",
    "\n",
    "print(f\"The number of correct preds was {n_correct_dev} for acc of {(n_correct_dev/dev_y_true.shape[0])*100}%\")\n",
    "print(f\"The number of pos preds was {(dev_y_pred==1).sum(axis=0)} and neg num was {(dev_y_pred==0).sum(axis=0)}\")\n",
    "\n",
    "# Additional detailed metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(dev_y_true, dev_y_pred)\n",
    "precision = precision_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "recall = recall_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "f1 = f1_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "cm = confusion_matrix(dev_y_true, dev_y_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Detailed Evaluation Metrics:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Fail    Pass\")\n",
    "print(f\"Actual Fail   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "print(f\"       Pass   {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "\n",
    "print(f\"\\nPredictions breakdown:\")\n",
    "print(f\"  Predicted Fail (0): {(dev_y_pred==0).sum()}\")\n",
    "print(f\"  Predicted Pass (1): {(dev_y_pred==1).sum()}\")\n",
    "print(f\"  Actual Fail (0): {(dev_y_true==0).sum()}\")\n",
    "print(f\"  Actual Pass (1): {(dev_y_true==1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ee25d-dc0c-4190-9dcd-2dc9fa9225dc",
   "metadata": {},
   "source": [
    "# After Midterm and before Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a5f7a-2ee5-4a90-9165-0dba15049870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged Perceptron: Train 10 perceptrons and average their weights\n",
    "n_perceptrons = 10\n",
    "weight_vecs = []\n",
    "bias_vecs = []\n",
    "features_lst = ['Attendance (%)', 'Extracurricular_Activities', 'Midterm_Score', \\\n",
    "       'Assignments_Avg', 'Quizzes_Avg', \\\n",
    "       'Participation_Score', \\\n",
    "       'Stress_Level (1-10)', \\\n",
    "       'Sleep_Hours_per_Night']\n",
    "num_feat = len(features_lst)\n",
    "batch_size=64\n",
    "shuffle = True\n",
    "\n",
    "for i in range(n_perceptrons):\n",
    "    train_ds = MyDataset(train_df, features_lst, 'Grade')\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    w_curr, b_curr, error_curr, weight_hist_curr, tot_train_pos_curr = train_perceptron(train_dl, num_feat, pos_class=1)\n",
    "    \n",
    "    weight_vecs.append(w_curr)\n",
    "    bias_vecs.append(b_curr)\n",
    "\n",
    "# Average the weights and biases\n",
    "w_avg = np.mean(weight_vecs, axis=0)\n",
    "b_avg = np.mean(bias_vecs, axis=0)\n",
    "\n",
    "print(f\"-------------------Averaged Perceptron (10 perceptrons)------------------------------\\n\")\n",
    "print(f\"Averaged weight vector shape: {w_avg}\")\n",
    "print(f\"Averaged bias value: {b_avg:.4f}\\n\")\n",
    "\n",
    "# Test on dev.\n",
    "X_dev_a = dev_df[features_lst].to_numpy()\n",
    "dev_y_true = dev_df['Grade'].to_numpy()\n",
    "dev_y_pred = ((X_dev_a @ w_avg + b_avg) > 0).astype(int)\n",
    "n_correct_dev = (dev_y_true==dev_y_pred).sum(axis=0)\n",
    "\n",
    "print(f\"The number of correct preds was {n_correct_dev} for acc of {(n_correct_dev/dev_y_true.shape[0])*100}%\")\n",
    "print(f\"The number of pos preds was {(dev_y_pred==1).sum(axis=0)} and neg num was {(dev_y_pred==0).sum(axis=0)}\")\n",
    "\n",
    "# Additional detailed metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(dev_y_true, dev_y_pred)\n",
    "precision = precision_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "recall = recall_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "f1 = f1_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "cm = confusion_matrix(dev_y_true, dev_y_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Detailed Evaluation Metrics:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Fail    Pass\")\n",
    "print(f\"Actual Fail   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "print(f\"       Pass   {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "\n",
    "print(f\"\\nPredictions breakdown:\")\n",
    "print(f\"  Predicted Fail (0): {(dev_y_pred==0).sum()}\")\n",
    "print(f\"  Predicted Pass (1): {(dev_y_pred==1).sum()}\")\n",
    "print(f\"  Actual Fail (0): {(dev_y_true==0).sum()}\")\n",
    "print(f\"  Actual Pass (1): {(dev_y_true==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8a928-6a3c-4c0c-bc9a-523a88141e52",
   "metadata": {},
   "source": [
    "# After Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eafc0b-00f4-422e-9f14-1a60c78849d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged Perceptron: Train 10 perceptrons and average their weights\n",
    "n_perceptrons = 10\n",
    "weight_vecs = []\n",
    "bias_vecs = []\n",
    "features_lst = ['Attendance (%)', 'Extracurricular_Activities', 'Midterm_Score', \\\n",
    "       'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', \\\n",
    "       'Participation_Score', 'Projects_Score', \\\n",
    "       'Stress_Level (1-10)', \\\n",
    "       'Sleep_Hours_per_Night']\n",
    "num_feat = len(features_lst)\n",
    "batch_size=64\n",
    "shuffle = True\n",
    "\n",
    "for i in range(n_perceptrons):\n",
    "    train_ds = MyDataset(train_df, features_lst, 'Grade')\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    w_curr, b_curr, error_curr, weight_hist_curr, tot_train_pos_curr = train_perceptron(train_dl, num_feat, pos_class=1)\n",
    "    \n",
    "    weight_vecs.append(w_curr)\n",
    "    bias_vecs.append(b_curr)\n",
    "\n",
    "# Average the weights and biases\n",
    "w_avg = np.mean(weight_vecs, axis=0)\n",
    "b_avg = np.mean(bias_vecs, axis=0)\n",
    "\n",
    "print(f\"-------------------Averaged Perceptron (10 perceptrons)------------------------------\\n\")\n",
    "print(f\"Averaged weight vector shape: {w_avg}\")\n",
    "print(f\"Averaged bias value: {b_avg:.4f}\\n\")\n",
    "\n",
    "# Test on dev.\n",
    "X_dev_a = dev_df[features_lst].to_numpy()\n",
    "dev_y_true = dev_df['Grade'].to_numpy()\n",
    "dev_y_pred = ((X_dev_a @ w_avg + b_avg) > 0).astype(int)\n",
    "n_correct_dev = (dev_y_true==dev_y_pred).sum(axis=0)\n",
    "\n",
    "print(f\"The number of correct preds was {n_correct_dev} for acc of {(n_correct_dev/dev_y_true.shape[0])*100}%\")\n",
    "print(f\"The number of pos preds was {(dev_y_pred==1).sum(axis=0)} and neg num was {(dev_y_pred==0).sum(axis=0)}\")\n",
    "\n",
    "# Additional detailed metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(dev_y_true, dev_y_pred)\n",
    "precision = precision_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "recall = recall_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "f1 = f1_score(dev_y_true, dev_y_pred, zero_division=0)\n",
    "cm = confusion_matrix(dev_y_true, dev_y_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Detailed Evaluation Metrics:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Fail    Pass\")\n",
    "print(f\"Actual Fail   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "print(f\"       Pass   {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "\n",
    "print(f\"\\nPredictions breakdown:\")\n",
    "print(f\"  Predicted Fail (0): {(dev_y_pred==0).sum()}\")\n",
    "print(f\"  Predicted Pass (1): {(dev_y_pred==1).sum()}\")\n",
    "print(f\"  Actual Fail (0): {(dev_y_true==0).sum()}\")\n",
    "print(f\"  Actual Pass (1): {(dev_y_true==1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f40813-920e-4707-bfed-f2398d86c486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
