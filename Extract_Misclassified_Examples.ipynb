{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Extracting Misclassified Examples from Averaged Perceptron\n",
    "\n",
    "trid to get:\n",
    "15 examples where the model predicted **PASS** but actual was **FAIL** (False Positives)\n",
    "10 example where the model predicted **FAIL** but actual was **PASS** (False Negatives)\n",
    "\n",
    "Used davids avf pperceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done runnin\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 1234\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"done runnin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-cleaning-header",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning (from Data_Cleaning_Code.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data-cleaning-cell",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'students_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m education_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstudents_clean.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m education_data.drop(\u001b[33m'\u001b[39m\u001b[33mParent_Education_Level\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[32m      6\u001b[39m education_data[\u001b[33m'\u001b[39m\u001b[33mGender\u001b[39m\u001b[33m'\u001b[39m] = education_data[\u001b[33m'\u001b[39m\u001b[33mGender\u001b[39m\u001b[33m'\u001b[39m].replace({\u001b[33m'\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m}).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/csc396/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/csc396/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/csc396/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/csc396/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/csc396/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'students_clean.csv'"
     ]
    }
   ],
   "source": [
    "education_data = pd.read_csv('students_clean.csv')\n",
    "\n",
    "\n",
    "education_data.drop('Parent_Education_Level', axis=1, inplace=True) \n",
    "\n",
    "education_data['Gender'] = education_data['Gender'].replace({'Male': 1, 'Female': 0}).astype(int)\n",
    "education_data['Internet_Access_at_Home'] = education_data['Internet_Access_at_Home'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "education_data['Extracurricular_Activities'] = education_data['Extracurricular_Activities'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "\n",
    "# Low = 1, Medium = 2, High = 3\n",
    "mapper = {'low': 1, 'medium': 2, 'high': 3}\n",
    "\n",
    "education_data['Family_Income_Level'] = (\n",
    "    education_data['Family_Income_Level']\n",
    "      .astype(str)                  # works even if the value is already 1/2/3 or NaN\n",
    "      .str.strip().str.lower()\n",
    "      .map(mapper)                  # returns NaN where no mapping found\n",
    "      .fillna(education_data['Family_Income_Level'])  # keepin the original numeric/blank entries\n",
    "      .astype('Int64')              #  nullable integer dtype\n",
    ")\n",
    "\n",
    "labels = open('departments.txt').read().splitlines()\n",
    "department_mapping = {name: index for index, name in enumerate(labels)}\n",
    "department_indices = education_data['Department'].map(department_mapping)\n",
    "education_data.insert(3, 'department index', department_indices)\n",
    "\n",
    "mapper = {'A': 1, 'B': 1, 'C': 1, 'D':0,'F':0}\n",
    "\n",
    "education_data['Grade'] = (\n",
    "    education_data['Grade']\n",
    "      .astype(str)              # convert everything to string\n",
    "      .str.strip().str.upper()  # remove spaces and standardize to uppercase\n",
    "      .map(mapper)              # map letters to numbers\n",
    ")\n",
    "\n",
    "education_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 3. Dataset Class and Perceptron (from our codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dataset-class-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class from perceptron notebook\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        x = torch.tensor(row[self.feature_cols].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target_col], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "perceptron-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(train_dl, n_features, pos_class):\n",
    "    # First initialize the model.\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    n_errors = 0\n",
    "    weight_steps = []\n",
    "    total_pos_in_train = 0\n",
    "\n",
    "    # Adding this in for debug purposes to track the changes to the weight vectors on each\n",
    "    # round.\n",
    "    \n",
    "    # Average perceptron features\n",
    "    totalW = np.zeros(n_features)\n",
    "    totalB = 0;\n",
    "    updateCount = 0;\n",
    "    \n",
    "    # Now loop through each batch.\n",
    "    for batch_idx, (x, y) in tqdm(enumerate(train_dl), total=len(train_dl),):\n",
    "        \n",
    "        x_curr_np = x.numpy()\n",
    "        y_curr_np = y.numpy()\n",
    "\n",
    "        total_pos_in_train += (y_curr_np == 1).sum(axis=0)\n",
    "        \n",
    "\n",
    "        # Now perform the training/classification loop.\n",
    "        scores = x_curr_np @ w + b\n",
    "       \n",
    "        \n",
    "        y_pred = (scores > 0).astype(int)\n",
    "\n",
    "\n",
    "        # Now we vectorize the update to make this more efficient.\n",
    "        pred_error = y_curr_np - y_pred\n",
    "        n_errors += np.sum(np.abs(pred_error) != 0) # If the pred error is zero then it is correct.\n",
    "\n",
    "        # First append the previous weights to weight steps which will be used for debuging puprposes.\n",
    "        weight_steps.append((pred_error[:,None]*x_curr_np).sum(axis=0).copy())\n",
    "        \n",
    "        w += (pred_error[:,None]*x_curr_np).sum(axis=0) # Re-shape pred errors to update and only add\n",
    "                                                        # inccorect preds, axis=0 for rows.\n",
    "        b += pred_error.sum()\n",
    "\n",
    "        # Now print out the weights and bias updates every update if we are in debug mode.\n",
    "        \n",
    "\n",
    "    # Now once we are done training the result is the weights and biases.\n",
    "    return (w,b,n_errors,weight_steps.copy(),total_pos_in_train) # I am just copying to avoid weird cases due to mutability of list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test-split-header",
   "metadata": {},
   "source": [
    "## 4. Create Train/Dev/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "train-test-split-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 3,600\n",
      "Dev rows: 900\n",
      "Test rows: 500\n",
      "\n",
      "Dev set Pass/Fail distribution:\n",
      "  Pass (1): 555\n",
      "  Fail (0): 345\n"
     ]
    }
   ],
   "source": [
    "# Split using same random state as our PassFail notebook\n",
    "train_df, test_df = train_test_split(education_data, train_size=0.9, random_state=seed)\n",
    "train_df, dev_df = train_test_split(train_df, train_size=0.8, random_state=seed)\n",
    "\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "dev_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f'Train rows: {len(train_df):,}')\n",
    "print(f'Dev rows: {len(dev_df):,}')\n",
    "print(f'Test rows: {len(test_df):,}')\n",
    "\n",
    "print(f\"\\nDev set Pass/Fail distribution:\")\n",
    "print(f\"  Pass (1): {(dev_df['Grade'] == 1).sum()}\")\n",
    "print(f\"  Fail (0): {(dev_df['Grade'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-avg-perceptron-header",
   "metadata": {},
   "source": [
    "## 5. Train Averaged Perceptron (10 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "train-avg-perceptron-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10 perceptrons...\n",
      "\n",
      "Training perceptron 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc885bff02e4f8494cd40b8c0443b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fad6ce79c74122a47a36ba3abec29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa11e4b0d7654a978ae480682598a92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863af5f6a6cc45509079c42f9f9d67fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5c16c6c0bd449c88b71ed9d23d5cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7118f1dd96f4a0485443c7d650cd470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360fc3b892b64c8bb1e6bd18c3744d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83826aec3a1b4ab2a76ba969fa6714bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7334c9a1ba074fa594797f71985356f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8396934a274ae4a1b878a3fc80e028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Averaged Perceptron Training Complete\n",
      "======================================================================\n",
      "Averaged weight vector shape: (10,)\n",
      "Averaged bias value: -97.3000\n"
     ]
    }
   ],
   "source": [
    "#using After Final features for most comprehensive results)\n",
    "n_perceptrons = 10\n",
    "weight_vecs = []\n",
    "bias_vecs = []\n",
    "\n",
    "\n",
    "features_lst = ['Attendance (%)', 'Extracurricular_Activities', 'Midterm_Score', \n",
    "                'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', \n",
    "                'Participation_Score', 'Projects_Score', \n",
    "                'Stress_Level (1-10)', 'Sleep_Hours_per_Night']\n",
    "num_feat = len(features_lst)\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "\n",
    "print(f\"Training {n_perceptrons} perceptrons...\\n\")\n",
    "\n",
    "for i in range(n_perceptrons):\n",
    "    print(f\"Training perceptron {i+1}/{n_perceptrons}\")\n",
    "    train_ds = MyDataset(train_df, features_lst, 'Grade')\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    w_curr, b_curr, error_curr, weight_hist_curr, tot_train_pos_curr = train_perceptron(train_dl, num_feat, pos_class=1)\n",
    "    \n",
    "    weight_vecs.append(w_curr)\n",
    "    bias_vecs.append(b_curr)\n",
    "\n",
    "# Average the weights and biases\n",
    "w_avg = np.mean(weight_vecs, axis=0)\n",
    "b_avg = np.mean(bias_vecs, axis=0)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Averaged Perceptron Training Complete\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Averaged weight vector shape: {w_avg.shape}\")\n",
    "print(f\"Averaged bias value: {b_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Dev Set and Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "evaluate-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Performance:\n",
      "  Accuracy: 69.33%\n",
      "  Correct predictions: 624/900\n",
      "\n",
      "Prediction Distribution:\n",
      "  Predicted Pass (1): 795\n",
      "  Predicted Fail (0): 105\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on dev set\n",
    "X_dev = dev_df[features_lst].to_numpy()\n",
    "dev_y_true = dev_df['Grade'].to_numpy()\n",
    "dev_y_pred = ((X_dev @ w_avg + b_avg) > 0).astype(int)\n",
    "\n",
    "# accuracy\n",
    "n_correct_dev = (dev_y_true == dev_y_pred).sum()\n",
    "accuracy = (n_correct_dev / dev_y_true.shape[0]) * 100\n",
    "\n",
    "print(f\"Dev Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Correct predictions: {n_correct_dev}/{len(dev_y_true)}\")\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "print(f\"  Predicted Pass (1): {(dev_y_pred == 1).sum()}\")\n",
    "print(f\"  Predicted Fail (0): {(dev_y_pred == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-misclassified-header",
   "metadata": {},
   "source": [
    "## 7. Extract Misclassified Examples\n",
    "False Positives (FP): Predicted PASS but actual was FAIL (15 examples)\n",
    "False Negatives (FN): Predicted FAIL but actual was PASS (10 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "extract-misclassified-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Misclassification Summary\n",
      "======================================================================\n",
      "False Positives (predicted Pass, actual Fail): 48\n",
      "False Negatives (predicted Fail, actual Pass): 323\n",
      "\n",
      "Extracting:\n",
      "  - 15 False Positives\n",
      "  - 10 False Negatives\n"
     ]
    }
   ],
   "source": [
    "# extract misclassifications\n",
    "# False Positives: Predicted Pass (1), Actual Fail (0)\n",
    "false_positives_mask = (dev_y_pred == 1) & (dev_y_true == 0)\n",
    "false_positives_indices = np.where(false_positives_mask)[0]\n",
    "\n",
    "# False Negatives: Predicted Fail (0), Actual Pass (1)\n",
    "false_negatives_mask = (dev_y_pred == 0) & (dev_y_true == 1)\n",
    "false_negatives_indices = np.where(false_negatives_mask)[0]\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Misclassification Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"False Positives (predicted Pass, actual Fail): {len(false_positives_indices)}\")\n",
    "print(f\"False Negatives (predicted Fail, actual Pass): {len(false_negatives_indices)}\")\n",
    "print(f\"\\nExtracting:\")\n",
    "print(f\"  - 15 False Positives\")\n",
    "print(f\"  - 10 False Negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fp-examples-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FALSE POSITIVES: Predicted PASS, Actually FAILED (15 examples)\n",
      "======================================================================\n",
      "\n",
      "     Gender  Age   Department  Attendance (%)  Midterm_Score  Final_Score  Assignments_Avg  Total_Score  Prediction_Score  Predicted_Grade  Actual_Grade\n",
      "8         0   24           CS           70.32          41.14        83.66            66.76      65.4480     311376.803550                1             0\n",
      "19        1   23  Engineering           52.96          78.38        55.22            60.21      67.2125     279356.486499                1             0\n",
      "31        1   19           CS           66.87          72.25        88.65            54.94      69.7470     289219.908868                1             0\n",
      "37        1   20  Engineering           65.60          76.57        75.55            54.99      61.2315     227794.060081                1             0\n",
      "57        1   22  Mathematics           61.57          76.65        58.67            56.18      68.5950     273010.801482                1             0\n",
      "75        0   22  Engineering           63.44          50.81        79.49            50.36      59.7920     250924.226629                1             0\n",
      "84        0   20  Engineering           66.26          48.70        91.29            57.82      69.6975     387554.409452                1             0\n",
      "135       0   23  Engineering           51.90          68.25        50.87            55.48      65.8030     299818.152908                1             0\n",
      "136       1   23  Engineering           50.61          83.63        76.12            67.12      66.8745     408799.812706                1             0\n",
      "140       1   21  Engineering           52.29          61.63        87.64            50.47      66.9520     387296.620129                1             0\n",
      "162       0   19  Engineering           51.15          45.94        74.18            59.72      65.6760     264149.296571                1             0\n",
      "166       1   21  Mathematics           60.60          70.21        64.48            87.85      68.0900     282062.601518                1             0\n",
      "172       1   22  Mathematics           55.70          66.72        41.12            60.90      66.2220     290905.630109                1             0\n",
      "175       0   18  Mathematics           54.78          56.68        57.49            56.47      60.9360     287810.839862                1             0\n",
      "208       0   20           CS           64.86          74.76        83.48            54.28      69.4350     286466.744867                1             0\n",
      "\n",
      "======================================================================\n",
      "False Positive Summary Statistics:\n",
      "======================================================================\n",
      "Average Total Score: 66.11\n",
      "Average Final Score: 71.19\n",
      "Average Attendance: 59.26%\n",
      "Average Prediction Score: 301769.7597\n"
     ]
    }
   ],
   "source": [
    "# xtract 15 False Positives\n",
    "num_fp_to_extract = min(15, len(false_positives_indices))\n",
    "fp_indices = false_positives_indices[:num_fp_to_extract]\n",
    "\n",
    "#  DataFrame with all relevant information\n",
    "false_positives_df = dev_df.iloc[fp_indices].copy()\n",
    "false_positives_df['Predicted_Grade'] = 1  # Pass\n",
    "false_positives_df['Actual_Grade'] = 0     # Fail\n",
    "false_positives_df['Error_Type'] = 'False Positive'\n",
    "\n",
    "# Add prediction scores for analysis\n",
    "false_positives_df['Prediction_Score'] = (X_dev[fp_indices] @ w_avg + b_avg)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FALSE POSITIVES: Predicted PASS, Actually FAILED ({num_fp_to_extract} examples)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Display with key features\n",
    "display_cols = ['Gender', 'Age', 'Department', 'Attendance (%)', 'Midterm_Score', \n",
    "                'Final_Score', 'Assignments_Avg', 'Total_Score', \n",
    "                'Prediction_Score', 'Predicted_Grade', 'Actual_Grade']\n",
    "\n",
    "print(false_positives_df[display_cols].to_string(index=True))\n",
    "\n",
    "#  stats\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"False Positive Summary Statistics:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Average Total Score: {false_positives_df['Total_Score'].mean():.2f}\")\n",
    "print(f\"Average Final Score: {false_positives_df['Final_Score'].mean():.2f}\")\n",
    "print(f\"Average Attendance: {false_positives_df['Attendance (%)'].mean():.2f}%\")\n",
    "print(f\"Average Prediction Score: {false_positives_df['Prediction_Score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fp-detailed-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FALSE NEGATIVES: Predicted FAIL, Actually PASSED (10 examples)\n",
      "======================================================================\n",
      "\n",
      "    Gender  Age   Department  Attendance (%)  Midterm_Score  Final_Score  Assignments_Avg  Total_Score  Prediction_Score  Predicted_Grade  Actual_Grade\n",
      "0        1   21           CS           92.88          45.43        51.62            94.52      70.5845      42801.751265                0             1\n",
      "2        1   21     Business           98.22          47.15        85.39            97.60      72.3400     163187.006902                0             1\n",
      "3        1   22     Business           80.57          99.83        87.61            90.90      82.2960     169692.011112                0             1\n",
      "4        1   23  Mathematics           81.60          44.80        75.08            91.32      76.7290     249189.706768                0             1\n",
      "5        0   24           CS           84.67          66.00        71.99            95.98      77.2365     176139.783562                0             1\n",
      "6        1   21           CS           89.28          49.40        55.45            99.21      72.6910      23592.634379                0             1\n",
      "7        1   21  Engineering           74.81          96.02        75.85            99.87      76.2540     139905.645494                0             1\n",
      "9        0   19  Engineering           98.37          69.31        83.73            59.24      74.7340      91500.239203                0             1\n",
      "15       1   22  Mathematics           83.19          98.26        46.12            73.90      70.0070      99506.278765                0             1\n",
      "18       1   22  Mathematics           89.21          70.30        77.87            53.95      70.3220     210231.395679                0             1\n",
      "\n",
      "======================================================================\n",
      "False Negative Summary Statistics:\n",
      "======================================================================\n",
      "Average Total Score: 74.32\n",
      "Average Final Score: 71.07\n",
      "Average Attendance: 87.28%\n",
      "Average Prediction Score: 136574.6453\n"
     ]
    }
   ],
   "source": [
    "# Extract 10 False Negatives\n",
    "num_fn_to_extract = min(10, len(false_negatives_indices))\n",
    "fn_indices = false_negatives_indices[:num_fn_to_extract]\n",
    "\n",
    "# Create DataFrame with all relevant information\n",
    "false_negatives_df = dev_df.iloc[fn_indices].copy()\n",
    "false_negatives_df['Predicted_Grade'] = 0  # Fail\n",
    "false_negatives_df['Actual_Grade'] = 1     # Pass\n",
    "false_negatives_df['Error_Type'] = 'False Negative'\n",
    "\n",
    "# Add prediction scores for analysis\n",
    "false_negatives_df['Prediction_Score'] = (X_dev[fn_indices] @ w_avg + b_avg)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FALSE NEGATIVES: Predicted FAIL, Actually PASSED ({num_fn_to_extract} examples)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Display with key features\n",
    "print(false_negatives_df[display_cols].to_string(index=True))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"False Negative Summary Statistics:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Average Total Score: {false_negatives_df['Total_Score'].mean():.2f}\")\n",
    "print(f\"Average Final Score: {false_negatives_df['Final_Score'].mean():.2f}\")\n",
    "print(f\"Average Attendance: {false_negatives_df['Attendance (%)'].mean():.2f}%\")\n",
    "print(f\"Average Prediction Score: {false_negatives_df['Prediction_Score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_perceptrons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAveraged Perceptron (trained with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mn_perceptrons\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m models)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features_lst)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDev set accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'n_perceptrons' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Averaged Perceptron (trained with {n_perceptrons} models)\")\n",
    "print(f\"Features used: {len(features_lst)}\")\n",
    "print(f\"Dev set accuracy: {accuracy:.2f}%\")\n",
    "print()\n",
    "print(f\"Misclassification Analysis:\")\n",
    "print(f\"  Total False Positives (Predicted PASS, Actual FAIL): {len(false_positives_indices)}\")\n",
    "print(f\"  Total False Negatives (Predicted FAIL, Actual PASS): {len(false_negatives_indices)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b650d9-5634-4de6-a38b-e2543a74e287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9451f1-4bdc-49e7-b17a-b88f94b35c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a914a-b4c9-4a71-ade9-446bc8af6c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
